---
title: "Multivariate Analysis Assignment 1"
author: "Harshad Kumar Elangovan - 19200349"
date: "29/02/2020"
output: html_document
---

Loading the Glass dataset and excluding a random row using the below code block.
```{r Loading Data}
glassData<-read.csv("Glass.csv",header = T)
set.seed(19200349)

num=floor(runif(1,min=1,max =nrow(glassData)))

glassData<-glassData[-num,]
```

## Question1

1 (a) (i) Compute the correlation matrix R of the 13 chemical elements using the fact that R = D-1/2SD-1/2 where S is the sample covariance matrix and D-1/2 is a diagonal matrix with the inverse of each variable's standard deviation on the diagonal. Ensure your calculation is correct by showing that your result and that computed by the cor function in R are equal. 

```{r 1a}
data1 <- glassData[,-1]
S <-c()
D<-c()
for (i in 1:ncol(data1)) {
  colmeans1<-mean(data1[,i])
  for (j in 1:ncol(data1)) {
    colmeans2<-mean(data1[,j])
    sum1 <- 0
    for (a in 1:nrow(data1)) {
      sum1<- sum1 + ((data1[a,i]-colmeans1)*(data1[a,j]-colmeans2))
    }
    S<-c(S,sum1/(nrow(data1)-1))
    if(i==j){
      D<-c(D,1/sqrt(sum1/(nrow(data1)-1)))
    }
    else{
      D<-c(D,0)
    }
    
  }
  
}
Covariance <- matrix(S,nrow = 13,ncol = 13,byrow = T)
Diagonal <- matrix(D,nrow = 13,ncol = 13,byrow = T)
Correlation<-Diagonal%*%Covariance%*%Diagonal
cat("The computed correlation is ")
print(Correlation)
cat("Correlation computed using R code is ")
RCorrelation<-cor(data1)
print(RCorrelation)


```

So, the output of both the variables are same and shows that the computation is correct.


(a) (ii) Using R, determine the first two eigenvalues and eigenvectors of the covariance matrix S. Using R, show that they are indeed eigenvalues and eigenvectors of S. 

```{r a2}
eigendata <- eigen(Covariance)
#Determining the first two eigen values and eigen vactors
eigendata$values[1:2]
eigendata$vectors[,c(1,2)]

#Verification
Covariance%*%eigendata$vectors[,1]
matrix(eigendata$values[1]%*%eigendata$vectors[,1])

#Both the output are same. So, it proves that they are eigen values and eigen vectors of S.

Covariance%*%eigendata$vectors[,2]
matrix(eigendata$values[2]%*%eigendata$vectors[,2])


```

(a) (iii) Using R, verify that the first two eigenvectors are orthonormal.

A set of vectors is defined orthogonal when
u.uj = 0 for i <> j

and it is defined orthonormal when also
ui.uj= 1 for i=j.

```{r a3}
eigendata$vectors[,1]
t(eigendata$vectors[,1])%*%eigendata$vectors[,1]
t(eigendata$vectors[,2])%*%eigendata$vectors[,2]
t(eigendata$vectors[,1])%*%(eigendata$vectors[,2])
t(eigendata$vectors[,2])%*%eigendata$vectors[,1]
```
 (a) (iv) Compute the variance of each element and produce a suitable summarising plot. Would you advise standardizing the glass data prior to analysis? Explain your reasoning. 

The variance can be computed from the co-variance matrix. 
```{r a4}
#The diagonal elements of S are the variance and that can be extracted to a variable for plotting.
variance<-diag(Covariance)
plot(variance,xlab = "Chemical Variance",ylab = "Variance",main = "Variance Plot")

```

From the plot, we can see that there are few abnormal points in the graph. From the data aswell, we can see that there are few columns which has high values comparatively to other columns. So we will have to standardize the dataset before creating clusters for an unbiased result. 

1 (b) Suppose E[X1] = 5 and V ar[X1] = 6. Suppose E[X2] = 8 and V ar[X2] = 7. Suppose also that the covariance between X1 and X2 is 2.5. 

(i) In R, calculate the expected value and variance of X2???X1. 

```{r 1b}
#Given Data Initialization
EX1<-5
EX2<-8
VarX1<-6
VarX2<-7
CovX1X2<-2.5

#Expected value of X2-X1 is EX2-EX1
EX2X1<-EX2-EX1

#Variance of X2-X1 is VX2+VX1-2CovX1X2
VarX2X1<- VarX1 + VarX2 -(2*CovX1X2)

cat("Expected Value of X2-X1 is ",EX2X1)
cat("Variance of X2-X1 is ",VarX2X1)
```

(b) (ii) In R, calculate the correlation of U = X2-X1 and V = X2-2X1. 

```{r b2}
#The Expected value and variance of U=X2-X1 is 
cat("Expected Value and Variance of X2-X1 is ",EX2X1,"and",VarX2X1)

#Expected value and Variance of V=X2-2X1 
EX2_2X1 <-EX2-(2*EX1)
VarX2_2X1<- VarX2 + (4*VarX1) - (4*CovX1X2)
cat("Expected Value and Variance of X2-2X1 is ",EX2_2X1,"and",VarX2_2X1)

#We have to find the covariance(X2-X1,X2-2X1) to calculate correlation by multiplying the matrix (1,-1)*(7,2.5,2.5,6)*t(1,-2)

totalcov<-matrix(c(1,-1),nrow=1,ncol = 2,byrow = T)%*%matrix(c(7,2.5,2.5,6),nrow = 2,ncol = 2,byrow = T)%*%matrix(c(1,-2),nrow = 2,ncol = 1,byrow = T)

CorrectionUV<-totalcov/sqrt(VarX2_2X1*VarX2X1)
cat("The correlation of U & V is ",CorrectionUV)
```

## Question 2

(a) In R, cluster the glass vessels using k-means clustering. How many clusters would you suggest are present in this data set? Detail any decisions you make when running this procedure. Provide details of your reasoning and fully commented R code

As we pointed out in 1(a)(iv), we will have to standardize the dataset before working on clusters.
```{r 2a}
sdval<-apply(data1,2,sd)
meanval<-apply(data1,2,mean)
data2<-scale(data1,meanval,sdval)
```

Now we can use it for exploring clusters.

```{r clustering}
k<-15
wss<-bss<-rep(NA,k)
for (i in 1:k) {
  fit1 <- kmeans(data2,centers = i,nstart = 50)
  wss[i]<-fit1$tot.withinss
  bss[i]<-fit1$betweenss
}
#computing CH Index 
N<-nrow(data1) 
ch<-(bss/(1:k-1))/(wss/(N-1:k)) 
ch[1]<-0
plot(1:k,ch,type = "b",ylab = "CH Values",xlab = "K Clusters", main="Calinski-Harabasz index")

```

From the plot, we can see that there is a shart point for CH values at K=2. After that, the line starts to decrease gradually with minimum upward. So there is an high possibility that there can be only two cluster for the given data. We can confirm this by comparing our dataset for clusters more than two.

```{r fits}
fit2<- kmeans(data2,centers = 2,nstart = 50)
fit3<- kmeans(data2,centers = 3,nstart = 50)
fit4<- kmeans(data2,centers = 4,nstart = 50)

table(fit2$cluster,glassData$Group)
table(fit3$cluster,glassData$Group)
table(fit4$cluster,glassData$Group)
```

The table of cluster 2 also looks fine compared with tables ofother cluster(k=3,k=4).

(b) Cluster the glass vessels using hierarchical clustering. Detail any decisions you make when conducting the hierarchical clustering. From the dendrogram, how many clusters would you suggest are present in this data set? Cut the dendrogram at the desired number of clusters.

First the distance between the data points is calculated using euclidean method and then the cluster are created using hiercharchical clustering.
```{r 2b}


distance<-dist(glassData,method = "euclidean")
c1.glassData<-hclust(distance,method = "average")
plot(c1.glassData)
#plot(c1.single)
#plot(c1.complete)
#c1.single<-hclust(distance,method = "single")
#c1.complete<-hclust(distance,method = "complete")

```

From the dendrogram taking the method for hierarchical clustering to "Average", we can see that the dendrogram can be divided to four clusters.

```{r cutree}
hierarchical2<-cutree(c1.glassData,k=2)
table(hierarchical2,glassData$Group)
hierarchical4<-cutree(c1.glassData,k=4)
table(hierarchical4,glassData$Group)
```

(c) Examine the cross tabulation of the cluster solutions obtained in (a) and (b) and then quantitatively compare them using an appropriate measure. Comment on the agreement between the two solutions. Provide details of your reasoning and fully commented R code.

```{r 2c}
#install.packages("e1071")
library("e1071")
#cross tabulation with 2 and four cluster
table2<-table(hierarchical2,fit2$cluster)
table4<-table(hierarchical4,fit4$cluster)
classAgreement(table2)
classAgreement(table4)
```

From the agreement, cluster 2 has the highest agreement compared with the model with four clusters. The Rand Index and Adjusted Rand Index of model with cluster two is 1 and the the Rand Index and Adjusted Rand Index of model with cluster four has values 0.67 and 0.40 respectively. From this, we can take clusters k=2 on highest priority than k=4.

(d) Create a pairs plot of the data, highlighting vessels from di???erent glass types using colour and/or plotting characters. Comment on the relative size of the ???rst glass type group and on the distribution of the PbO variable. Explore the impact of removing these data from your analysis. Why would detecing such issues be challenging in a truly unsupervised and high-dimensional setting?

```{r 2d}
symb<-c(15,16,17,18) 
col<-c("darkorange2","deepskyblue3","magenta3","burlywood1")
plot(data1,gap=0,pch=symb[glassData$Group],col=adjustcolor(col[glassData$Group],0.4),main="Group of Glasses")
```

From the plot, we can clearly see that one of the group is highly dominating the data since it is huge in numbers compared with other groups.

Relative size of the first group:

```{r group1size}
nrow(glassData[glassData$Group==1,])
```

So, this group 1 has the highest data in the dataset and it was clearly visible in the above pair plot.

```{r distpb0}
hist(glassData$PbO,xlab = "Pb0",ylab = "Frequency",col="chocolate2",main = "Distribution of Pb0")
```

The histgram shows that majority of the values of Pb0 was in between 0 and 0.5 and the graph is positively skewed. After that there are small data values of Pb0.

Impact of removing group 1 and Pb0 data and analyzing the data again.

```{r nogrp1Pb0}
glassdata2<-data1[glassData$Group!="1",-13]
fitnew2<-kmeans(glassdata2,centers = 2,nstart = 50)
fitnew3<-kmeans(glassdata2,centers = 3,nstart = 50)
plot(glassdata2,gap=0,pch=symb[1:3],col=adjustcolor(col[1:3],0.4),main="Group of Glasses")
table(fitnew2$cluster,glassData[glassData$Group!="1","Group"])
table(fitnew3$cluster,glassData[glassData$Group!="1","Group"])

```

From both the tables, we can see that the model with cluster 3 has better segregation of this new data and provides a clear view of the clusters after removing the Group1 and Pb0 variable. But,in a truly unsupervised and high-dimensional setting, we cannot confirm the actual groups and detecting the impact is very difficult for each variables.

## Question 4


![Answer4](C:/Users/harshie/Documents/UCD/Multivariate/Assignment1/q4.jpeg)


## Question 3

Load the MASS library in R, and its fgl dataset. Use the help file to understand what the data set contains. Decide whether or not you need to scale the data. Write your own function to calculate the misclassification error for linear discriminant analysis applied to the first 9 variables in this dataset, using the final variable as the known class. Split the data such that floor(n*(2/3)) observations are in the training set and the remainder in the test set, compute the misclassification rate, and repeat this 100 times. Create a suitable plot to illustrate the misclassification rates for each class and the overall misclassification rates. What is the average overall misclassification rate?

```{r loadingQ3data}
library(MASS)
data("fgl")
set.seed(19200349)
num1=floor(runif(1,min=1,max =nrow(fgl)))
fgldata<-fgl[-num1,]
nrow(fgl)
nrow(fgldata)
#help("fgl")
```

There are few data points in the dataset which are abnormally high compared with other variables.So, we have to standardize the dataset for unbiased calculation.

```{r standardize}
sdval1<-apply(fgldata[,-10],2,sd)
fgldata1<-sweep(fgldata[-10],2,sdval1,"/")
fgldata1<-data.frame(cbind(fgldata1,fgldata[,10]))
colnames(fgldata1)<-colnames(fgldata)
```

The data is now standardized and is further used for computing misclassification rate. LDA command will produce the prior and mean values that will be used for computing the misclassification rate.

```{r misclassification, error = TRUE}
misclasification<- function(dataval1,iteration){
  type<-c()
  for(i in 1:iteration){
    dataval<-sort(sample(nrow(dataval1),floor(nrow(data)*(2/3))))
    traindata<-dataval1[dataval,]
    testdata<-dataval1[-dataval,]
    covval<-c()
    lda.res<-lda(dataval1$type~dataval1$RI + dataval1$Na + dataval1$Mg + dataval1$Al + dataval1$Si + dataval1$K + dataval1$Ca + dataval1$Ba + dataval1$Fe)
    for(m in unique(traindata$type)){
      dataval2<-traindata[traindata$type==m,]
      covval<-c(covval,list(cov(dataval2[,-10])*nrow(dataval2)))
    }
    wgt_cov<-matrix(rep(0,81),nrow = 9,ncol = 9)
    for (n in 1:length(unique(traindata$type))) {
      wgt_cov<-wgt_cov + covval[[n]]
    }
    wgt_cov<-wgt_cov/nrow(traindata)
    test_class<-c()
    for (f in rownames(testdata)) {
      test_class_2<-c()
      for (g in 1:length(unique(traindata$type))) {
        test_class_2<-c(test_class_2,log(lda.res$prior[g])+as.matrix(testdata[f,-10])%*%inv(wgt_cov)%*%matrix(lda.res$mean[g]))- ((1/2)*(t(matrix(lda.res$mean[g])%*%inv(wgt_cov)%*%matrix(lda.res$mean[g]))))
      
      }
      test_class<-c(test_class,test_class_2)
    }
    test_class<-data.frame(matrix(test_class,nrow = nrow(testdata),ncol = length(unique(traindata$type)),byrow = T))
    colnames(test_class)<-unique(traindata$type)
    
    test_class<-colnames(test_class)[apply(test_class,1,which.max)]
    newdata<-data.frame(cbind(as.character(testdata$type),test_class))
    colnames(newdata)<-c("Actual","Observed")
    new_class<-c()
    new_class<-c(new_class,i)
    
    for(l in unique(traindata$type)){
      correct_count<-nrow(subset(newdata,Actual==l & Observed!=l))
      total_count<-nrow(newdata[newdata[,"Actual"]==l,])
      if(total_count!=0){
        new_class<-c(new_class,(correct_count/total_count)*100)
      }
      else{
        new_class<-c(new_class,0)
      }
    }
    count<-0
    for(h in 1:nrow(new)){
      if(new[1,"Actual"]!=new[1,"Observed"]){
      count<count+1
      }
    }
  new_class<-c(new_class,(count/nrow(new))*100)
  type<-c(type,new_class)
}
type<-data.frame(matrix(type,nrow=iteration,ncol = length(unique(dataval1$type))+2,byrow = T))
colnames(type)<-c("Iteration","WinF","WinNF","Veh","Con","Tabl","Head","Overall")
return(type)
}

fglclass<-misclasification(fgldata1,100)
attach(fglclass)
#class.long<-melt(fglclass,id="Iteration",measure=c("WinF","WinNF","Veh","Con","Tabl","Head","Overall"))

```
